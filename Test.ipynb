{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def search_matrix(matrix, target):\n",
    "    if not matrix or not matrix[0]:\n",
    "        return False\n",
    "\n",
    "    rows, cols = len(matrix), len(matrix[0])\n",
    "    left, right = 0, rows * cols - 1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        mid_element = matrix[mid // cols][mid % cols]\n",
    "\n",
    "        if mid_element == target:\n",
    "            return True\n",
    "        elif mid_element < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "matrix = [[1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 60]]\n",
    "target = 3\n",
    "output = search_matrix(matrix, target)\n",
    "print(output)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm treats the 2D matrix as a flattened 1D array and performs a binary search on it. \n",
    "The conversion between 1D and 2D indices is done using the formulas mid // cols and mid % cols. \n",
    "The time complexity of this algorithm is O(log(m * n)), where m is the number of rows and n is the \n",
    "number of columns in the matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here's a Python program that takes a string as input, \n",
    "counts the frequency of each word, and returns the length of the highest-frequency word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def highest_frequency_word_length(string):\n",
    "    words = string.split()\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    if not word_counts:\n",
    "        return 0  # Empty string case\n",
    "    \n",
    "    max_frequency = max(word_counts.values())\n",
    "    \n",
    "    # Find the first word with the maximum frequency\n",
    "    highest_frequency_word = next(word for word, count in word_counts.items() if count == max_frequency)\n",
    "    \n",
    "    return len(highest_frequency_word)\n",
    "\n",
    "# Example usage\n",
    "example_input = \"write write write all the number from from from 1 to 100\"\n",
    "output = highest_frequency_word_length(example_input)\n",
    "print(output)  # Output: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The program uses the Counter class from the collections module to count the frequency of each word in the input string.\n",
    "It then finds the word with the maximum frequency and returns the length of that word.\n",
    "Now, let's add two more test cases:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test Case 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "test_input = \"apple orange banana apple orange banana apple\"\n",
    "output = highest_frequency_word_length(test_input)\n",
    "print(output)  # Output: 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "In this case, \"apple\" and \"orange\" both have the same maximum frequency,\n",
    "but the program returns the length of the first word with the maximum frequency, \n",
    "which is \"apple\" with a length of 6.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "test_input = \"programming is fun programming is creative programming is exciting\"\n",
    "output = highest_frequency_word_length(test_input)\n",
    "print(output)  # Output: 11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "    Here, \"programming\" is the word with the highest frequency, and its length is 11.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Data Preprocessing:\n",
    "\n",
    "      Handle missing values: Check for and handle any missing values in the dataset.Feature engineering: Extract relevant information from features like time posted, create numerical representations for categorical variables, etc.\n",
    "        Convert text data: If you have text data like Username, Caption, or Hashtag, you might need to use techniques like tokenization or word embeddings to convert them into numerical representations.\n",
    "        \n",
    "2) Split the Data:\n",
    "       Split the dataset into training and testing sets to evaluate the model's performance.\n",
    "\n",
    "3) Feature Scaling:\n",
    "\n",
    "     Normalize or standardize numerical features to ensure that they are on a similar scale, which can improve the model's performance.\n",
    "    Model Selection:\n",
    "Choose appropriate models for regression tasks. Common models include linear regression, decision trees, random forests, or more complex models like neural networks.\n",
    "\n",
    "4) Model Training:\n",
    "     Train the selected models on the training dataset.\n",
    "\n",
    "5) Model Evaluation:\n",
    "    Evaluate the model's performance on the testing dataset using appropriate metrics for regression tasks,\n",
    "    such as Mean Absolute Error (MAE) or Mean Squared Error (MSE).\n",
    "\n",
    "6) Hyperparameter Tuning:\n",
    "     Fine-tune the model's hyperparameters to improve its performance.\n",
    "\n",
    "7) Predictions:\n",
    "     Make predictions on new data using the trained model.\n",
    "\n",
    "        \n",
    "Here is a simple example using Python with scikit-learn:\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing machine learning (ML) in a real-world application involves several steps, \n",
    "from data preparation to deploying the trained model. \n",
    "Lets go through the process of implementing an ML model using a Support Vector Machine (SVM) \n",
    "regressor on the Bengaluru housing dataset.\n",
    "\n",
    "Steps to Implement ML in a Real-world Application:\n",
    "1. Define the Problem:\n",
    "Clearly define the problem you want to solve. In this case, it's likely predicting housing prices in Bengaluru based on various features.\n",
    "2. Gather Data:\n",
    "Obtain a dataset that is representative of the problem. In this case, you've mentioned the Bengaluru housing dataset.\n",
    "3. Data Exploration and Preprocessing:\n",
    "Explore the dataset to understand its structure, features, and target variable.\n",
    "Handle missing values, outliers, and perform feature engineering if necessary.\n",
    "Split the dataset into training and testing sets.\n",
    "4. Choose a Model:\n",
    "Based on the nature of your problem (regression, classification, etc.), choose an appropriate model. In this case, you've mentioned SVM regression.\n",
    "5. Feature Scaling:\n",
    "SVM models are sensitive to the scale of input features, so scale the features using techniques like Standard Scaling.\n",
    "6. Train the Model:\n",
    "Use the training data to train the SVM regressor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Assume df is your DataFrame containing the housing dataset\n",
    "X = df.drop('target_variable', axis=1)  # Features\n",
    "y = df['target_variable']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM regressor\n",
    "svm_regressor = SVR(kernel='linear')  # You can experiment with different kernels\n",
    "svm_regressor.fit(X_train_scaled, y_train)\n",
    "7. Evaluate the Model:\n",
    "Use the testing data to evaluate the model's performance.\n",
    "python\n",
    "Copy code\n",
    "# Make predictions\n",
    "predictions = svm_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "8. Hyperparameter Tuning (Optional):\n",
    "Fine-tune the model's hyperparameters to improve performance.\n",
    "9. Deploy the Model:\n",
    "Once satisfied with the model's performance, deploy it in a real-world application.\n",
    "10. Monitor and Maintain:\n",
    "Regularly monitor the model's performance and retrain it as needed to maintain accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution link = \"https://colab.research.google.com/drive/1dK002fQYV-gJmRDQ_iYLkcXzMfndJVOn#scrollTo=wpn-m7CghlF2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the Problem:\n",
    "a. Identify the Problem:\n",
    "Clearly define the problem you want to solve using DL. It could be a classification task, regression task, object detection, natural language processing, etc.\n",
    "b. Set Objectives and Metrics:\n",
    "Establish clear objectives for your DL model. Define metrics to measure the model's success, such as accuracy, precision, recall, F1-score, or mean squared error, depending on the problem.\n",
    "2. Data Collection and Preprocessing:\n",
    "a. Gather Data:\n",
    "Collect a representative dataset for training and testing your DL model. Ensure the data is diverse and covers the scenarios you want the model to handle.\n",
    "b. Data Cleaning and Exploration:\n",
    "Perform data cleaning to handle missing values, outliers, and inconsistencies.\n",
    "Explore the data to gain insights into its distribution and characteristics.\n",
    "c. Data Preprocessing:\n",
    "Preprocess the data based on the requirements of your DL model. This may include normalization, scaling, handling categorical variables, and data augmentation for image data.\n",
    "3. Model Selection and Design:\n",
    "a. Choose DL Architecture:\n",
    "Select an appropriate DL architecture based on your problem. Common architectures include Convolutional Neural Networks (CNNs) for image data, Recurrent Neural Networks (RNNs) for sequential data, and various architectures for different natural language processing tasks.\n",
    "b. Design the Model:\n",
    "Design the architecture of your DL model using frameworks like TensorFlow or PyTorch. Define the number of layers, activation functions, and other hyperparameters.\n",
    "4. Model Training:\n",
    "a. Split the Data:\n",
    "Split the dataset into training, validation, and test sets.\n",
    "b. Train the Model:\n",
    "Use the training set to train your DL model. Adjust hyperparameters and monitor performance on the validation set.\n",
    "c. Hyperparameter Tuning:\n",
    "Fine-tune hyperparameters to improve model performance.\n",
    "5. Model Evaluation:\n",
    "a. Evaluate on Test Set:\n",
    "Evaluate the trained model on the test set to assess its generalization to unseen data.\n",
    "b. Analyze Results:\n",
    "Analyze the model's performance using the predefined metrics. Identify areas for improvement.\n",
    "6. Deployment:\n",
    "a. Choose Deployment Environment:\n",
    "Decide where and how the DL model will be deployed. This could be on cloud platforms, edge devices, or on-premises servers.\n",
    "b. Deployment Tools:\n",
    "Use deployment tools like TensorFlow Serving, TensorFlow Lite, ONNX, or Docker containers to deploy your DL model.\n",
    "c. Monitor and Update:\n",
    "Implement monitoring to track the model's performance in real-time. Be prepared to update the model as new data becomes available or the environment changes.\n",
    "7. Maintenance and Continuous Improvement:\n",
    "a. Regular Updates:\n",
    "Periodically update the model to adapt to changes in the data distribution or to incorporate new features.\n",
    "b. Feedback Loop:\n",
    "Establish a feedback loop to gather insights from users or stakeholders. Use this feedback to improve the model.\n",
    "c. Security and Privacy:\n",
    "Implement measures to ensure the security and privacy of the deployed DL model, especially when dealing with sensitive data.\n",
    "8. Documentation:\n",
    "a. Document Model Architecture:\n",
    "Provide detailed documentation on the architecture, training process, and deployment procedures.\n",
    "b. Model Versioning:\n",
    "Implement version control for models to keep track of changes and improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
